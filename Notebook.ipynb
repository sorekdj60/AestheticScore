{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def round_to_half(score):\n",
    "    \"\"\"Round the float to the nearest half (.0 or .5)\"\"\"\n",
    "    return round(score * 2) / 2\n",
    "\n",
    "# Directory where the images are stored\n",
    "image_dir = 'path_to_your_images_directory'\n",
    "\n",
    "# Read the content of the text file\n",
    "with open(\"your_file.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process each line to extract filename and score\n",
    "for line in lines:\n",
    "    filename, score = line.strip().split()\n",
    "    rounded_score = round_to_half(float(score))  # Apply rounding to nearest half\n",
    "\n",
    "    # Create the score folder if it doesn't exist\n",
    "    score_folder = os.path.join(image_dir, str(rounded_score))\n",
    "    if not os.path.exists(score_folder):\n",
    "        os.makedirs(score_folder)\n",
    "    \n",
    "    # Create 'image' and 'output' subfolders\n",
    "    image_subfolder = os.path.join(score_folder, 'image')\n",
    "    output_subfolder = os.path.join(score_folder, 'output')\n",
    "\n",
    "    if not os.path.exists(image_subfolder):\n",
    "        os.makedirs(image_subfolder)\n",
    "    if not os.path.exists(output_subfolder):\n",
    "        os.makedirs(output_subfolder)\n",
    "\n",
    "    # Move the image file into the 'image' subfolder\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    new_image_path = os.path.join(image_subfolder, filename)\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        shutil.move(image_path, new_image_path)\n",
    "    else:\n",
    "        print(f\"Image {filename} not found!\")\n",
    "\n",
    "    # Create JSON output for this file\n",
    "    json_data = {\"filename\": filename, \"score\": rounded_score}\n",
    "    json_output_path = os.path.join(output_subfolder, f\"{filename}.json\")\n",
    "\n",
    "    with open(json_output_path, \"w\") as json_file:\n",
    "        json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "image_dir = 'Images'\n",
    "\n",
    "# Read the content of the text file\n",
    "with open(\"label.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process each line to extract filename and score\n",
    "for line in lines:\n",
    "    filename, score = line.strip().split()\n",
    "    rounded_score = int(float(score))  # Apply rounding to nearest half\n",
    "\n",
    "    # Create the score folder if it doesn't exist\n",
    "    score_folder = os.path.join(image_dir, str(rounded_score))\n",
    "    if not os.path.exists(score_folder):\n",
    "        os.makedirs(score_folder)\n",
    "    \n",
    "    # Create 'image' and 'output' subfolders\n",
    "    image_subfolder = score_folder\n",
    "\n",
    "    if not os.path.exists(image_subfolder):\n",
    "        os.makedirs(image_subfolder)\n",
    "\n",
    "\n",
    "    # Move the image file into the 'image' subfolder\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    new_image_path = os.path.join(image_subfolder, filename)\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        shutil.copy(image_path, new_image_path)\n",
    "    else:\n",
    "        print(f\"Image {filename} not found!\")\n",
    "\n",
    "    # Create JSON output for this file\n",
    "    json_data = {\"filename\": filename, \"score\": rounded_score}\n",
    "\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Example transforms, modify as needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "datasets_path = 'Datasets'\n",
    "full_dataset = datasets.ImageFolder(root=datasets_path, transform=transform)\n",
    "\n",
    "def dataset_spliter(full_dataset, prozent_test=10, prozent_val=10, batch_size=32):\n",
    "    if isinstance(prozent_test, int):\n",
    "        prozent_test = prozent_test / 100\n",
    "    if isinstance(prozent_val, int):\n",
    "        prozent_val = prozent_val / 100\n",
    "    \n",
    "    # Calculate sizes for train, validation, and test\n",
    "    test_size = int(prozent_test * len(full_dataset))\n",
    "    val_size = int(prozent_val * len(full_dataset))\n",
    "    train_size = len(full_dataset) - test_size - val_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Example usage\n",
    "train_loader, val_loader, test_loader = dataset_spliter(full_dataset=full_dataset, prozent_test=10, prozent_val=10, batch_size=32)\n",
    "idx_to_class = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
    "num_classes = len(idx_to_class.keys())\n",
    "class_names = list(idx_to_class.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sorel Tahata Djoumsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sorel Tahata Djoumsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Chargement du modèle ResNet18 pré-entraîné\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train the final layer\n",
    "model.fc = nn.Linear(model.fc.in_features, len(class_names))\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Définition de la fonction de perte et de l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sorel Tahata Djoumsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Epoch 1/50: 100%|██████████| 138/138 [00:11<00:00, 11.87batch/s, accuracy=0.633, loss=0.977]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.8966, Train Acc: 0.6332, Val Loss: 0.8219, Val Acc: 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 138/138 [00:11<00:00, 11.85batch/s, accuracy=0.697, loss=0.63] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.7327, Train Acc: 0.6970, Val Loss: 0.7597, Val Acc: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 138/138 [00:11<00:00, 11.82batch/s, accuracy=0.704, loss=0.789]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.7113, Train Acc: 0.7036, Val Loss: 0.7599, Val Acc: 0.6891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 138/138 [00:11<00:00, 11.81batch/s, accuracy=0.726, loss=0.391]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.6699, Train Acc: 0.7261, Val Loss: 0.7344, Val Acc: 0.6945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 138/138 [00:11<00:00, 11.74batch/s, accuracy=0.717, loss=1.02] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.6586, Train Acc: 0.7168, Val Loss: 0.6974, Val Acc: 0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 138/138 [00:11<00:00, 11.72batch/s, accuracy=0.737, loss=0.702]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.6485, Train Acc: 0.7370, Val Loss: 0.7068, Val Acc: 0.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 138/138 [00:11<00:00, 11.85batch/s, accuracy=0.738, loss=0.696]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.6416, Train Acc: 0.7380, Val Loss: 0.7298, Val Acc: 0.7091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 138/138 [00:11<00:00, 11.68batch/s, accuracy=0.732, loss=0.509]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.6347, Train Acc: 0.7318, Val Loss: 0.6837, Val Acc: 0.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 138/138 [00:11<00:00, 11.69batch/s, accuracy=0.738, loss=0.783]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.6205, Train Acc: 0.7375, Val Loss: 0.7011, Val Acc: 0.7182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 138/138 [00:11<00:00, 11.64batch/s, accuracy=0.743, loss=1.01] \n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.6194, Train Acc: 0.7434, Val Loss: 0.6938, Val Acc: 0.7182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 138/138 [00:11<00:00, 11.83batch/s, accuracy=0.744, loss=0.568]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.6080, Train Acc: 0.7443, Val Loss: 0.6962, Val Acc: 0.7164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 138/138 [00:11<00:00, 11.86batch/s, accuracy=0.747, loss=0.829]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.6084, Train Acc: 0.7466, Val Loss: 0.6943, Val Acc: 0.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 138/138 [00:11<00:00, 11.84batch/s, accuracy=0.763, loss=0.605]\n",
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.5798, Train Acc: 0.7627, Val Loss: 0.6886, Val Acc: 0.7200\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 0.7218\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001, patience=5):\n",
    "    # Set device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop with tqdm progress bar\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for inputs, labels in tepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Update progress bar with current loss and accuracy\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=correct / total)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        # Validation loop with tqdm progress bar\n",
    "        with tqdm(val_loader, unit=\"batch\", leave=False) as vepoch:\n",
    "            vepoch.set_description(f\"Validation\")\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in vepoch:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            model.load_state_dict(best_model_state)  # Restore the best model\n",
    "            break\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = correct / total\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, num_epochs=50, patience=5)\n",
    "\n",
    "# Test the model\n",
    "test_model(model, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), 'beauti_classificator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
